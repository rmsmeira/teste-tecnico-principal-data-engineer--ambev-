{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b10824-8e3d-45ba-b9e2-86c9d5828c4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "library"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e180ea-fc53-467c-ba30-7f74787a2853",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "beverage_channel_group"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"/Volumes/rodrigo_lakehouse/bronze/brz_vol_ambev/abi_bus_case1_beverage_channel_group/\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    encoding=\"UTF-8\"\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rodrigo_lakehouse.bronze.ambev_channel_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e8c57c-f7c0-4573-ab56-c2bd3ae3e229",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "beverage_sales"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(\n",
    "    \"/Volumes/rodrigo_lakehouse/bronze/brz_vol_ambev/abi_bus_case1_beverage_sales/\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"UTF-16\"\n",
    ")\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df = df.select([col(c).alias(c.replace(\" \", \"_\").replace(\"$\", \"DOLLAR\")) for c in df.columns])\n",
    "\n",
    "# Write the DataFrame to a Delta table with overwrite mode and enable schema evolution\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rodrigo_lakehouse.bronze.ambev_beverage_sales\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4638549763319489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "landing_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
